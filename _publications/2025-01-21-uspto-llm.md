---
title: "Multi-order Orchestrated Curriculum Distillation for Model-Heterogeneous Federated Graph Learning"
collection: publications
permalink: /publication/uspto-llm
venue: 'The Thirty-ninth Annual Conference on Neural Information Processing Systems (NeurIPS 2025)'
paperurl: 'https://openreview.net/forum?id=C9653lXaFO&referrer=%5Bthe%20profile%20of%20Xu%20Cheng%5D(%2Fprofile%3Fid%3D~Xu_Cheng5)'
category: 'Federated Learning'
#citation: 'Your Name, You. (2024). &quot;Paper Title Number 3.&quot; <i>GitHub Journal of Bugs</i>. 1(3).'
---

<!-- Over the past few years, the machine learning community has given increasing attention to chemical reaction prediction and retrosynthesis. Despite impressive achievements, the existing datasets in this field have gradually become the bottleneck of current research â€” the limitation of dataset size and the lack of reaction condition information hinder the practicability of the current methods. In this study, we construct an information-enriched chemical reaction dataset called USPTO-LLM, with the help of large language models (LLMs). This dataset comprises over 247K chemical reactions extracted from the patent documents of USPTO (United States Patent and Trademark Office), encompassing abundant information on reaction conditions. We employ large language models to expedite the data collection procedures automatically with a reliable quality control process. Experiments show that USPTO-LLM helps pre-train the existing retrosynthesis methods and the condition information in the dataset helps improve the model performance. The dataset is open-sourced at [https://zenodo.org/records/14396156](https://zenodo.org/records/14396156) and the annotation code is open-sourced at [https://github.com/GONGSHUKAI/USPTO_LLM](https://github.com/GONGSHUKAI/USPTO_LLM). -->