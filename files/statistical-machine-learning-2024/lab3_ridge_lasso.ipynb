{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Regularized Linear Regression\n",
    "Covered Topics:\n",
    "> Ridge Regression\n",
    "\n",
    "> SGD for Ridge Regression\n",
    "\n",
    "> LASSO and Iterative Soft-thresholding\n",
    "\n",
    "> Elastic Net Regularization\n",
    "\n",
    "> Iterative Reweighted Least Squares (IRLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "> Implement the commonly-used regularization methods for learning regression.\n",
    "\n",
    "> Implement commonly-used learning algorithm, such as soft-thresholding for lasso, and iteratively reweighted least squares (IRLS) for robust linear regression.\n",
    "\n",
    "> Quantitatively analyze the performance of different methods on model estimation and data prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Simulator and Testing Function (Don't change them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# don't add any other packages\n",
    "\n",
    "\n",
    "# data simulator and testing function (Don't change them)\n",
    "def linear_data_simulator(n_train: int = 50,\n",
    "                          n_test: int = 10,\n",
    "                          dim: int = 10,\n",
    "                          v_noise: float = 2,\n",
    "                          prior: str = 'Gauss',\n",
    "                          noise: str = 'Gauss',\n",
    "                          r_seed: int = 42) -> dict:\n",
    "    \"\"\"\n",
    "    Simulate the training and testing data generated by a polynomial model\n",
    "    :param n_train: the number of training data\n",
    "    :param n_test: the number of testing data\n",
    "    :param dim: the dimension of feature\n",
    "    :param v_noise: the hyper-parameter controlling the variance of data noise\n",
    "    :param prior: the prior distribution of model parameters, Gauss or Laplace\n",
    "    :param noise: the noise type, Gauss or Laplace\n",
    "    :param r_seed: the random seed\n",
    "    :return:\n",
    "        a dictionary containing training set, testing set, and the ground truth parameters\n",
    "    \"\"\"\n",
    "    x_train = np.random.RandomState(r_seed).rand(n_train, dim)\n",
    "    x_test = np.random.RandomState(r_seed).rand(n_test, dim)\n",
    "    if prior == 'Gauss':\n",
    "        weights = np.random.RandomState(r_seed).randn(dim, 1)\n",
    "    else:\n",
    "        weights = np.random.RandomState(r_seed).laplace(\n",
    "            loc=0, scale=1, size=(dim, 1))\n",
    "    if noise == 'Gauss':\n",
    "        y_train = x_train @ weights + v_noise * \\\n",
    "            np.random.RandomState(r_seed).randn(n_train, 1)\n",
    "        y_test = x_test @ weights + v_noise * \\\n",
    "            np.random.RandomState(r_seed).randn(n_test, 1)\n",
    "    else:\n",
    "        y_train = x_train @ weights + \\\n",
    "            np.random.RandomState(r_seed).laplace(\n",
    "                loc=0, scale=v_noise, size=(n_train, 1))\n",
    "        y_test = x_test @ weights + \\\n",
    "            np.random.RandomState(r_seed).laplace(\n",
    "                loc=0, scale=v_noise, size=(n_test, 1))\n",
    "    data = {'train': [x_train, y_train],\n",
    "            'test': [x_test, y_test],\n",
    "            'real': weights}\n",
    "    return data\n",
    "\n",
    "\n",
    "def mse(x: np.ndarray, x_est: np.ndarray) -> float:\n",
    "    return np.sum((x - x_est) ** 2) / x.shape[0]\n",
    "\n",
    "\n",
    "def testing(x: np.ndarray, y: np.ndarray, weights: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute the MSE of regression based on current model\n",
    "    :param x: testing data with size (N, )\n",
    "    :param y: testing label with size (N, 1)\n",
    "    :param weights: model parameter with size (D, 1)\n",
    "    :return:\n",
    "        MSE\n",
    "    \"\"\"\n",
    "    y_est = x @ weights\n",
    "    return mse(y, y_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "Given the problem\n",
    "$$\n",
    "\\min_{\\boldsymbol{w}} \\|\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{w}\\|_2^2 + \\gamma \\|\\boldsymbol{w}\\|_2^2\n",
    "$$\n",
    "implement the function “training” to achieve its closed-form solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution 1\n",
    "The closed form solution of **Ridge Regression** is given by\n",
    "$$\n",
    "    \\dfrac{\\partial L}{\\partial \\boldsymbol{w}}=-2\\boldsymbol{X}^\\top (\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{w})+2\\gamma \\boldsymbol{w}=0\\Rightarrow \\boldsymbol{w}=(\\boldsymbol{X}^\\top \\boldsymbol{X}+\\gamma \\boldsymbol{I})^{-1}\\boldsymbol{X}^\\top \\boldsymbol{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(x: np.ndarray, y: np.ndarray, gamma: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    The training function of ridge regression model\n",
    "\n",
    "    min_w ||y - Xw||_2^2 + gamma * ||w||_2^2\n",
    "\n",
    "    :param x: input data with size (N, dim)\n",
    "    :param y: labels of data with size (N, 1)\n",
    "    :param gamma: the weight of the ridge regularizer\n",
    "    :return:\n",
    "        a weight vector with size (dim, 1)\n",
    "    \"\"\"\n",
    "    inv = np.linalg.inv(x.T @ x + gamma * np.eye(x.shape[1]))\n",
    "    return inv @ x.T @ y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little example that examines our correctiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.96345515]\n",
      " [1.0268801 ]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "y = np.array([[3], [7], [11]])\n",
    "gamma = 0.1\n",
    "# this should be [[1], [1]] because y = x1 + x2\n",
    "\n",
    "weights = training(x, y, gamma)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "Given the problem in (1), implement the function “training_sgd” to learn the model via stochastic gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution 2\n",
    "Similarily, we only need to change the gradient for each iteration in the SGD algorithm.\n",
    "> Polynomial regression: $\\dfrac{\\partial L}{\\partial \\boldsymbol{w}}=2\\boldsymbol{X}^\\top_B(\\boldsymbol{X}_B\\boldsymbol{w}_{t-1}-\\boldsymbol{y}_B)$.\n",
    "\n",
    "> Ridge regression: $\\dfrac{\\partial L}{\\partial \\boldsymbol{w}}=2\\boldsymbol{X}^\\top_B(\\boldsymbol{X}_B\\boldsymbol{w}_{t-1}-\\boldsymbol{y}_B)+2\\gamma \\boldsymbol{w}_{t-1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_sgd(x: np.ndarray,\n",
    "                 y: np.ndarray,\n",
    "                 gamma: float,\n",
    "                 epoch: int = 10,\n",
    "                 batch_size: int = 10,\n",
    "                 lr: float = 1e-4,\n",
    "                 r_seed: int = 1) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    The stochastic gradient descent method of ridge regression.\n",
    "    :param x: input data with size (N, dim)\n",
    "    :param y: labels of data with size (N, 1)\n",
    "    :param gamma: the weight of ridge regularizer\n",
    "    :param epoch: the number of epochs\n",
    "    :param batch_size: the batch size for sgd\n",
    "    :param lr: the learning rate\n",
    "    :param r_seed: random seed\n",
    "    :return:\n",
    "        a weight vector with size (order, 1)\n",
    "    \"\"\"\n",
    "    np.random.seed(r_seed)\n",
    "    # Number of samples\n",
    "    n = x.shape[0]\n",
    "    # Dimension of the input\n",
    "    dim = x.shape[1]\n",
    "    weights = np.random.randn(dim, 1)\n",
    "    for e in range(epoch):\n",
    "        idx = np.random.permutation(n)\n",
    "        x = x[idx]\n",
    "        y = y[idx]\n",
    "        for i in range(0, n, batch_size):\n",
    "            x_batch = x[i:i+batch_size]\n",
    "            y_batch = y[i:i+batch_size]\n",
    "            grad = (-2 * x_batch.T @ (y_batch - x_batch @ weights) + 2 * gamma * weights) / batch_size\n",
    "            weights -= lr * grad\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little example that examines our correctiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight: \n",
      "['2.082887016911663', '0.1431523566821604']\n",
      "estimated y: \n",
      "['2.369191730275984', '6.8212704774636315', '11.273349224651277']\n",
      "real y: \n",
      "['3', '7', '11']\n"
     ]
    }
   ],
   "source": [
    "weight_sgd = training_sgd(x, y, gamma, epoch=10000)\n",
    "print(\"weight: \")\n",
    "print([str(weight_sgd[0]) for weight_sgd in weight_sgd])\n",
    "print(\"estimated y: \")\n",
    "print([str((x[i] @ weight_sgd)[0]) for i in range(len(x))])\n",
    "print(\"real y: \")\n",
    "print([str((y[i])[0]) for i in range(len(y))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "Implement the iterative soft-thresholding algorithm to solve the lasso problem\n",
    "$$\n",
    "\\min_{\\boldsymbol{w}} \\|\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{w}\\|_2^2 + \\gamma \\|\\boldsymbol{w}\\|_1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution 3\n",
    "First we investiget the closed-form solution of LASSO Regression under **orthonormal condition**, i.e. $\\boldsymbol{X}^\\top \\boldsymbol{X}=\\boldsymbol{I_D}$. This is a special case but serves as a good starting point for understanding the general case.\n",
    "\n",
    "The closed-form solution of LASSO Regression under **soft-thresholding** can written as\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        w_{d}^*=S_{\\lambda}(w_{OLS,d})=\\text{sign}(w_{OLS,d})\\cdot \\max\\{|w_{OLS,d}|-\\lambda,0\\},\\ d=1,2,\\cdots,D\n",
    "    \\end{align*}\n",
    "$$\n",
    "with each $w_d^*$ being the $d$-th element of the solution $\\boldsymbol{w}^*$. Here, $w_{OLS,d}$ is the $d$-th element of the OLS solution $\\boldsymbol{w}_{OLS}=(\\boldsymbol{X}^\\top \\boldsymbol{X})^{-1}\\boldsymbol{X}^\\top \\boldsymbol{y}$.\n",
    "\n",
    "The $S_{\\lambda}(\\cdot)$ is the soft-thresholding function with $\\lambda$ being the threshold, which we will implement first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_thresholding(x: np.ndarray, thres: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    The soft-thresholding operator\n",
    "    :param x: input array with arbitrary size\n",
    "    :param thres: the threshold\n",
    "    :return:\n",
    "        sign(x) * max{0, |x|-thres}\n",
    "    \"\"\"\n",
    "    return np.sign(x) * np.maximum(np.abs(x) - thres, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0, -1,  2,  3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1, 2, -3, 4, 5])\n",
    "thres = 2\n",
    "# this should be [0, 0, -1, 2, 3]\n",
    "soft_thresholding(x, thres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More generally, when $\\boldsymbol{X}^\\top \\boldsymbol{X}\\ne \\boldsymbol{I_D}$, we can construct orthonormal vectors column-wisely and update parameters iteratively. In the $t$-th iteration, it can be proved that the $d$-th parameter can be updated by **soft-thresholding**:\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        w_{d}^{(t+1)}=S_{\\frac{\\lambda}{\\|\\boldsymbol{x}_d\\|_2^2}}\\left(\\dfrac{\\boldsymbol{x}_d^\\top (\\boldsymbol{y}-\\boldsymbol{X}_{-d}\\boldsymbol{w}_{-d}^{(t)})}{\\|\\boldsymbol{x_d}\\|_2^2}\\right)\n",
    "    \\end{align*}\n",
    "$$\n",
    "It's worth noticing that we are actually updating $w_d^{(t+1)}$ with $w_1^{(t+1)},w_2^{(t+1)},\\cdots,w_{d-1}^{(t+1)}$ and $w_{d+1}^{(t)},\\cdots,w_D^{(t)}$. So the weight vector $\\boldsymbol{w}^{(t+1)}$ is updated column-wisely and **non-concurrently**.\n",
    "\n",
    "In the actual coding implementation, $\\boldsymbol{y}-\\boldsymbol{X}_{-d}\\boldsymbol{w}_{-d}^{(t)}$ is computed by\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        \\boldsymbol{y}-\\boldsymbol{X}_{-d}\\boldsymbol{w}_{-d}^{(t)}=\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{w}^{(t)}+\\boldsymbol{x}_d w_d^{(t)}\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_lasso(x: np.ndarray,\n",
    "                   y: np.ndarray,\n",
    "                   gamma: float,\n",
    "                   iteration: int = 100,\n",
    "                   r_seed: int = 1) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    The training function of lasso regression model\n",
    "\n",
    "    min_w ||y - Xw||_2^2 + gamma * ||w||_1\n",
    "\n",
    "    :param x: input data with size (N, dim)\n",
    "    :param y: labels of data with size (N, 1)\n",
    "    :param gamma: the weight of the lasso regularizer\n",
    "    :param iteration: the number of iterations for soft-thresholding\n",
    "    :param r_seed: the random seed for initializing model.\n",
    "    :return:\n",
    "        a weight vector with size (dim, 1)\n",
    "    \"\"\"\n",
    "    num = x.shape[0]\n",
    "    dim = x.shape[1]\n",
    "    np.random.seed(r_seed)\n",
    "    weights = np.random.randn(dim, 1)\n",
    "    for t in range(iteration):\n",
    "        for d in range(dim):\n",
    "            # x_d is x[:, d] (taking out the d-th column)\n",
    "            x_d = x[:, d]\n",
    "            xd2_square = np.linalg.norm(x_d) ** 2\n",
    "            # threshold is gamma / ||x_d||^2\n",
    "            threshold = gamma / xd2_square\n",
    "            # update the d-th weight\n",
    "            # w_hat = x_d^T(y - x_{-d}w_{-d}) / ||x_d||^2\n",
    "            #       = x_d^T(y - xw + x_d w_d) / ||x_d||^2\n",
    "            w_hat = x_d.T @ (y - x @ weights + (x_d * weights[d]).reshape(num,1)) / xd2_square\n",
    "            # soft-thresholding\n",
    "            weights[d] = soft_thresholding(w_hat, threshold)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.95  ],\n",
       "       [1.0375]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "y = np.array([[3], [7], [11]])\n",
    "gamma = 0.1\n",
    "# this should be [[1], [1]] because y = x1 + x2\n",
    "weights = training_lasso(x, y, gamma, iteration=10000)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Detail]: Don't forget `.reshape(num,1)` in `w_hat = x_d.T @ (y - x @ weights + (x_d * weights[d]).reshape(num,1)) / xd2_square` because the random weight that we generate is (1,D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4\n",
    "Implement an algorithm to solve the linear regression with elastic net regularization\n",
    "$$\n",
    "\\min_{\\boldsymbol{w}} \\|\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{w}\\|_2^2 + \\gamma_1 \\|\\boldsymbol{w}\\|_1 + \\gamma_2 \\|\\boldsymbol{w}\\|_2^2\n",
    "$$\n",
    "(Hint: Reformulate it to Lasso.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 4\n",
    "The basic idea is to integrate the $L_2$ regularization term into MSE $\\|\\boldsymbol{y-Xw}\\|_2^2$ and then apply the iterative soft-thresholding w.r.t. $L_1$ regularization. Note that the term $\\lambda_2 \\|\\boldsymbol{w}\\|_2^2$ can be written as \n",
    "$$\n",
    "    \\begin{align*}\n",
    "        \\lambda_2 \\|\\boldsymbol{w}\\|_2^2=\\|\\sqrt{\\lambda_2}\\boldsymbol{w}\\|_2^2=\\|\\boldsymbol{0}_D-\\sqrt{\\lambda_2}\\boldsymbol{I_D}\\boldsymbol{w}\\|_2^2\n",
    "    \\end{align*}\n",
    "$$\n",
    "where $\\boldsymbol{0}_D$ is a $D$-dimensional zero vector and $\\boldsymbol{I_D}$ is a $D\\times D$ identity matrix. We can therefore integrate the $L_2$ regularization term into the MSE term as\n",
    "$$ \n",
    "    \\begin{align*}\n",
    "        &\\|\\boldsymbol{y-Xw}\\|_2^2+\\|\\boldsymbol{0}_D-\\sqrt{\\lambda_2}\\boldsymbol{I_D}\\boldsymbol{w}\\|_2^2+\\lambda_1 \\|\\boldsymbol{w}\\|_1\\\\\n",
    "        =&\\left\\|\\begin{bmatrix}\n",
    "            \\boldsymbol{y}\\\\\n",
    "            \\boldsymbol{0}_D\n",
    "        \\end{bmatrix}-\\begin{bmatrix}\n",
    "            \\boldsymbol{X}\\\\\n",
    "            \\sqrt{\\lambda_2}\\boldsymbol{I_D}\n",
    "        \\end{bmatrix}\\boldsymbol{w} \\right\\|_2^2+\\lambda_1\\|\\boldsymbol{w}\\|_1\n",
    "    \\end{align*}\n",
    "$$\n",
    "And this goes back to $L_1$ regularization situation and we can put new matrix\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        \\boldsymbol{X'}=\\begin{bmatrix}\n",
    "            \\boldsymbol{X}\\\\\n",
    "            \\sqrt{\\lambda_2}\\boldsymbol{I_D}\n",
    "        \\end{bmatrix}, \\boldsymbol{y'}=\\begin{bmatrix}\n",
    "            \\boldsymbol{y}\\\\\n",
    "            \\boldsymbol{0}_D\n",
    "        \\end{bmatrix}\n",
    "    \\end{align*}\n",
    "$$\n",
    "into the iterative soft-thresholding solver for LASSO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_elastic(x: np.ndarray,\n",
    "                     y: np.ndarray,\n",
    "                     gamma1: float,\n",
    "                     gamma2: float,\n",
    "                     iteration: int = 100,\n",
    "                     r_seed: int = 1) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    The training function of lasso regression model\n",
    "\n",
    "    min_w ||y - Xw||_2^2 + gamma1 * ||w||_1 + gamma2 * ||w||_2^2\n",
    "\n",
    "    :param x: input data with size (N, dim)\n",
    "    :param y: labels of data with size (N, 1)\n",
    "    :param gamma1: the weight of the lasso regularizer\n",
    "    :param gamma2: the weight of the ridge regularizer\n",
    "    :param iteration: the number of iterations for soft-thresholding\n",
    "    :param r_seed: the random seed for initializing model.\n",
    "    :return:\n",
    "        a weight vector with size (dim, 1)\n",
    "    \"\"\"\n",
    "    dim = x.shape[1]\n",
    "    x_concat = np.concatenate((x, np.sqrt(gamma2) * np.eye(dim)), axis=0)\n",
    "    y_concat = np.concatenate((y, np.zeros(dim).reshape(dim, 1)))\n",
    "    return training_lasso(x_concat, y_concat, gamma1, iteration, r_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9269103 ],\n",
       "       [1.05376019]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "y = np.array([[3], [7], [11]])\n",
    "gamma1 = 0.1\n",
    "gamma2 = 0.1\n",
    "# this should be [[1], [1]] because y = x1 + x2\n",
    "weights = training_elastic(x, y, gamma1, gamma2, iteration=10000)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5\n",
    "Implement the iteratively reweighted least squares (IRLS) to solve robust linear regression:\n",
    "$$\n",
    "\\min_{\\boldsymbol{w}} \\|\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{w}\\|_1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 5\n",
    "Denote $\\alpha_n(\\boldsymbol{w}^{(t)})=|y_n-\\boldsymbol{x}_n^\\top \\boldsymbol{w}^{(t)}|^{-1}$ with $\\alpha_n(\\boldsymbol{w}^{(0)})=1$, the $t$-th iteration of IRLS is given by\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        \\boldsymbol{w}^{(t+1)}&=\\arg\\min\\limits_{\\boldsymbol{w}}\\sum\\limits_{n=1}^{N}\\alpha_n(\\boldsymbol{w}^{(t)})|y_n-\\boldsymbol{x}_n^\\top \\boldsymbol{w}|^2\n",
    "    \\end{align*}\n",
    "$$\n",
    "Denote $\\boldsymbol{\\alpha}(\\boldsymbol{w}^{(t)})=\\begin{bmatrix}\n",
    "        \\alpha_1(\\boldsymbol{w}^{(t)})\\\\\n",
    "        \\alpha_2(\\boldsymbol{w}^{(t)})\\\\\n",
    "        \\vdots\\\\\n",
    "        \\alpha_N(\\boldsymbol{w}^{(t)})\n",
    "    \\end{bmatrix}$, where $\\alpha_n(\\boldsymbol{w}^{(t)})$ is the $t$-th iteration of $n$-th dimension, we can write the objective function as\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        \\boldsymbol{w}^{(t+1)}&=\\arg\\min\\limits_{\\boldsymbol{w}}\\sum\\limits_{n=1}^{N}\\left(\\sqrt{\\alpha_n(\\boldsymbol{w}^{(t)})}\\right)^2|y_n-\\boldsymbol{x}_n^\\top \\boldsymbol{w}|^2\\\\\n",
    "        &=\\arg\\min\\limits_{\\boldsymbol{w}}\\|\\mathrm{diag}\\left(\\boldsymbol{\\alpha}(\\boldsymbol{w}^{(t)})\\right)^{\\frac{1}{2}}\\cdot(\\boldsymbol{y}-\\boldsymbol{Xw})\\|_2^2=\\arg\\min\\limits_{\\boldsymbol{w}}\\|(\\boldsymbol{A}^{(t)})^{\\frac{1}{2}}(\\boldsymbol{y}-\\boldsymbol{Xw})\\|_2^2\n",
    "    \\end{align*}\n",
    "$$\n",
    "where $\\boldsymbol{A}^{(t)}=\\mathrm{diag}(\\boldsymbol{\\alpha}(\\boldsymbol{w}^{(t)}))=\\begin{bmatrix}\n",
    "        \\dfrac{1}{|y_1-\\boldsymbol{x}_1^\\top \\boldsymbol{w}^{(t)}|}&  \\cdots & 0\\\\\n",
    "        \\vdots & \\ddots & \\vdots\\\\\n",
    "        0 &  \\cdots & \\dfrac{1}{|y_N-\\boldsymbol{x}_N^\\top \\boldsymbol{w}^{(t)}|}\n",
    "    \\end{bmatrix}$. To derive the closed-form iterative steps of IRLS, we first denote \n",
    "$$    \n",
    "\\begin{align*}\n",
    "    L&=\\|(\\boldsymbol{A}^{(t)})^{\\frac{1}{2}}(\\boldsymbol{y}-\\boldsymbol{Xw})\\|=(\\boldsymbol{y}^\\top-w^\\top \\boldsymbol{X}^\\top)((\\boldsymbol{A}^{(t)})^{\\frac{1}{2}})^\\top (\\boldsymbol{A}^{(t)})^{\\frac{1}{2}}(\\boldsymbol{y}-\\boldsymbol{Xw})\\\\\n",
    "    &=(\\boldsymbol{y}^\\top-w^\\top \\boldsymbol{X}^\\top)\\boldsymbol{A}^{(t)}(\\boldsymbol{y}-\\boldsymbol{Xw})\\\\\n",
    "    &=\\boldsymbol{y}^\\top \\boldsymbol{A}^{(t)}\\boldsymbol{y}-\\boldsymbol{y}^\\top \\boldsymbol{A}^{(t)}\\boldsymbol{Xw}-\\boldsymbol{w}^\\top \\boldsymbol{X}^\\top \\boldsymbol{A}^{(t)}\\boldsymbol{y}+\\boldsymbol{w}^\\top \\boldsymbol{X}^\\top \\boldsymbol{A}^{(t)}\\boldsymbol{Xw}\\\\\n",
    "    \\dfrac{\\partial L}{\\partial \\boldsymbol{w}}&=-2\\boldsymbol{X}^\\top \\boldsymbol{A}^{(t)}\\boldsymbol{y}+2\\boldsymbol{X}^\\top \\boldsymbol{A}^{(t)}\\boldsymbol{Xw}=0\\\\\n",
    "    \\Rightarrow \\boldsymbol{w}^{(t+1)}&=(\\boldsymbol{X}^\\top \\boldsymbol{A}^{(t)}\\boldsymbol{X})^{-1}\\boldsymbol{X}^\\top \\boldsymbol{A}^{(t)}\\boldsymbol{y}\n",
    "\\end{align*}\n",
    "$$\n",
    "Since we know $\\boldsymbol{w}^{(t)}$ and $\\boldsymbol{A}^{(t)}$, we can calculate $\\boldsymbol{w}^{(t+1)}$ iteratively.\n",
    "\n",
    "**Explanation:** \n",
    "> The idea of IRLS is to simply treat $\\boldsymbol{w}^*=\\arg\\min\\limits_{\\boldsymbol{w}}\\sum\\limits_{n=1}^{N}|y_n-\\boldsymbol{x}_n^\\top \\boldsymbol{w}|^2$, but with a weight $\\alpha_n(\\boldsymbol{w}^{(t)})$ attached to each $n$. The initial weights are set as $\\boldsymbol{\\alpha}=\\boldsymbol{1}$. \\textbf{In each iteration, we can lower an observation's importance (i.e. weight) by $\\alpha_n(\\boldsymbol{w}^{(t)})=|y_n-\\boldsymbol{x}_n^\\top \\boldsymbol{w}^{(t)}|^{-1}$ if it has a large residual (i.e. outliners).}\n",
    "\n",
    "> Naturally, IRLS works for p-norm with $p<2$: $\\alpha_n^{(t)}=|y_n-\\boldsymbol{x}_n^\\top \\boldsymbol{w}^{(t)}|^{p-2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_irls(x: np.ndarray,\n",
    "                  y: np.ndarray,\n",
    "                  iteration: int = 100,\n",
    "                  r_seed: int = 1) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    min_w ||y - Xw||_1 (MAE minimization)\n",
    "\n",
    "    :param x: testing data with size (N, dim)\n",
    "    :param y: testing label with size (N, 1)\n",
    "    :param iteration: the number of iterations for soft-thresholding\n",
    "    :param r_seed: the random seed for initializing model.\n",
    "    :return:\n",
    "        a weight vector with size (dim, 1)\n",
    "    \"\"\"\n",
    "    np.random.seed(r_seed)\n",
    "    N = x.shape[0]\n",
    "    dim = x.shape[1]\n",
    "    a = np.ones(N)\n",
    "    w = np.random.randn(dim, 1)\n",
    "    for t in range(iteration):\n",
    "        # Update vector a^(t) and construct matrix A^(t)\n",
    "        for n in range(N):\n",
    "            a[n] = 1 / abs(y[n] - x[n].T @ w)\n",
    "        A = np.diag(a)\n",
    "        # Update w^(t+1)\n",
    "        w = np.linalg.inv(x.T @ A @ x) @ x.T @ A @ y\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "y = np.array([[3], [7], [11]])\n",
    "# this should be [[1], [1]] because y = x1 + x2\n",
    "weights = training_irls(x, y, iteration=10000)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge regression, mse-w=0.5294, mse-y=2.6081\n",
      "Ridge regression (SGD), mse-w=1.8516, mse-y=18.4436\n",
      "Lasso regularizer, mse-w=0.7350, mse-y=2.4981\n",
      "Elastic Net regularizer, mse-w=0.5270, mse-y=2.7996\n",
      "IRLS, mse-w=0.9128, mse-y=5.2916\n"
     ]
    }
   ],
   "source": [
    "data1 = linear_data_simulator(prior='Gauss', noise='Gauss')\n",
    "data2 = linear_data_simulator(prior='Laplace', noise='Gauss')\n",
    "data3 = linear_data_simulator(prior='Gauss', noise='Laplace')\n",
    "w1 = training(data1['train'][0], data1['train'][1], gamma=1)\n",
    "w2 = training_sgd(data1['train'][0], data1['train'][1], gamma=1)\n",
    "w3 = training_lasso(data2['train'][0], data2['train'][1], gamma=1)\n",
    "w4 = training_elastic(\n",
    "    data2['train'][0], data2['train'][1], gamma1=1, gamma2=1)\n",
    "w5 = training_irls(data3['train'][0], data3['train'][1])\n",
    "\n",
    "mse_w1 = mse(data1['real'], w1)\n",
    "mse_w2 = mse(data1['real'], w2)\n",
    "mse_w3 = mse(data2['real'], w3)\n",
    "mse_w4 = mse(data2['real'], w4)\n",
    "mse_w5 = mse(data3['real'], w5)\n",
    "\n",
    "mse_y1 = testing(data1['test'][0], data1['test'][1], w1)\n",
    "mse_y2 = testing(data1['test'][0], data1['test'][1], w2)\n",
    "mse_y3 = testing(data2['test'][0], data2['test'][1], w3)\n",
    "mse_y4 = testing(data2['test'][0], data2['test'][1], w4)\n",
    "mse_y5 = testing(data3['test'][0], data3['test'][1], w5)\n",
    "\n",
    "print(\n",
    "    'Ridge regression, mse-w={:.4f}, mse-y={:.4f}'.format(mse_w1, mse_y1))\n",
    "print(\n",
    "    'Ridge regression (SGD), mse-w={:.4f}, mse-y={:.4f}'.format(mse_w2, mse_y2))\n",
    "print(\n",
    "    'Lasso regularizer, mse-w={:.4f}, mse-y={:.4f}'.format(mse_w3, mse_y3))\n",
    "print(\n",
    "    'Elastic Net regularizer, mse-w={:.4f}, mse-y={:.4f}'.format(mse_w4, mse_y4))\n",
    "print('IRLS, mse-w={:.4f}, mse-y={:.4f}'.format(mse_w5, mse_y5))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
